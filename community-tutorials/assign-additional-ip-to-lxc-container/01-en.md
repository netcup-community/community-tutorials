---
title: How to Assign Additional IPs to LXC Containers
description: Learn how to use additional IPs with LXC containers, by installing a BigBlueButton server as an example.
level: [advanced]
updated_at: 2022-01-05
slug: assign-additional-ip-to-lxc-container
author_name: Dashamir Hoxha
author_url: http://dashohoxha.fs.al
author_image: -
author_bio: -
tags: [lxc, container, bigbluebutton, coturn] 
netcup_product_url: https://www.netcup.eu/bestellen/produkt.php?produkt=1228
language: en
available_languages: [en]
---

# Introduction

I have a [root server](https://www.netcup.eu/vserver/) on netcup.eu,
with ubuntu:20.04, and inside it I want to install
[BigBlueButton](https://bigbluebutton.org/).  The latest stable
release of BBB requires ubuntu:18.04, so I'd like to install it inside
an LXC container. For ease of installation, the script
[bbb-install.sh](https://github.com/bigbluebutton/bbb-install) is
recommended, which works best if used in a system with public IP (not
a NAT-ed one). For this reason I have also [purchased a second
IP](https://www.netcup.eu/bestellen/produkt.php?produkt=1228) from
netcup, which I want to use for the BBB container. Netcup routes the
second IP to the primary one. I'd like also to install a TURN server
in another container, which may improve the connectivity of the
clients to the BBB server.


# Requirements

For a production BBB server, an [RS 4000
G9](https://www.netcup.eu/bestellen/produkt.php?produkt=2628) should
be OK. But if you don't expect it to be used heavily, even a smaller
server or a VPS should be fine. See [more details about
requirements](https://docs.bigbluebutton.org/2.4/install.html#minimum-server-requirements)
of the BBB docs.


# Step 1 - Installing LXD

[LXD](https://linuxcontainers.org/lxd/introduction/) is used to manage
the LXC containers. I installed it like this:

```bash
apt install snap
snap install lxd --channel=4.0/stable
snap list
lxc list
lxd init
```

The output from the last command looks like this:

```bash
Would you like to use LXD clustering? (yes/no) [default=no]:
Do you want to configure a new storage pool? (yes/no) [default=yes]:
Name of the new storage pool [default=default]:
Name of the storage backend to use (btrfs, dir, lvm, zfs, ceph) [default=zfs]: btrfs
Create a new BTRFS pool? (yes/no) [default=yes]:
Would you like to use an existing empty block device (e.g. a disk or partition)? (yes/no) [default=no]:
Size in GB of the new loop device (1GB minimum) [default=30GB]: 70
Would you like to connect to a MAAS server? (yes/no) [default=no]:
Would you like to create a new local network bridge? (yes/no) [default=yes]:
What should the new bridge be called? [default=lxdbr0]:
What IPv4 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]:
What IPv6 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]:
Would you like the LXD server to be available over the network? (yes/no) [default=no]:
Would you like stale cached images to be updated automatically? (yes/no) [default=yes]
Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]:
```

For the storage backend I am using **btrfs** because I need to install
docker inside the containers, and only this filesystem supports it.


# Step 2 - Removing the second IP from the host

The default configuration assigns the second IP to eth0 on the
host. I removed it like this:

```bash
ip addr del 37.121.182.6/32 dev eth0
```

To remove it permanently, I edited `/etc/netplan/50-cloud-init.yaml`
and commented out the second IP line. Then applied the changes:
`netplan apply`


# Step 3 - Creating a profile for the container

```bash
lxc profile create bbb
lxc profile edit bbb
lxc profile list
```

The content of the `bbb` profile looks like this:

```yaml
config:
  security.nesting: true
  user.network-config: |
    version: 2
    ethernets:
        eth0:
            addresses:
            - 37.121.182.6/32
            nameservers:
                addresses:
                - 8.8.8.8
                search: []
            routes:
            -   to: 0.0.0.0/0
                via: 169.254.0.1
                on-link: true
description: Routed LXD profile
devices:
  eth0:
    ipv4.address: 37.121.182.6
    nictype: routed
    parent: eth0
    host_name: veth-bbb
    type: nic
name: bbb
used_by:
```

The configuration **`security.nesting: true`** is needed in order to
run docker inside the container. However if the container is
unprivileged it does not really have any security implications.

I am using the second public IP that I deleted from the host interface
(`37.121.182.6/32`). For this reason this profile can be used only for
one container. To build other containers like this, we should make a
copy of the profile and modify it.

The **`config.user.network-config`** part is about the configuration
of the container (through cloud-init). The gateway is `169.254.0.1`.

Notice that **`devices.eth0.nictype`** is **`routed`**. I could have
used an **ipvlan** type as well, and most of the configurations would
be almost the same, however it seems that in this case the container
cannot ping the public IP of the host, and I don't want this.

The field **`devices.eth0.host_name`** sets the name of the virtual
interface that will be created on the host. If it is not specified,
then a random name will be used each time the container is
started. But this would make difficult the specification of the
firewall rules (that we will see later).

The field **`devices.eth0.parent`** is the name of the interface on
the host where this virtual interface will be attached. In my case
this is not really necessary and can be left out or commented.

# Step 4 - Launching the container

The latest stable version of BBB requires `ubuntu:18.04`.

```bash
lxc launch ubuntu:18.04 bbb --profile default --profile bbb
lxc list
lxc list -c ns4t
lxc info bbb
```

With the command `ip addr` you can notice that a new interface named
`veth-bbb` has been created, and it has the IP `169.254.0.1/32`.

With the command `ip ro` you can notice that a route like this has
been added:

```
37.121.182.6 dev veth-bbb scope link
```

Try also these commands:

```bash
lxc exec bbb -- ip addr
lxc exec bbb -- ip ro
```

Notice that the interface inside the container has IP
`37.121.182.6/32` and the default gateway is `169.254.0.1`.

We can also ping from the host to `37.121.182.6`, however from the
container we cannot ping to the host or outside (to the Internet):

```bash
ping 37.121.182.6
lxc exec bbb -- ping 169.254.0.1
lxc exec bbb -- ping 8.8.8.8
```

# Step 5 - Fix networking

The problem is that I have installed `firewalld` on the host and it
blocks these connections. To fix this problem we can add the interface
that is connected to the container to the **trusted** zone of the
firewall:

```bash
firewall-cmd --add-interface=veth-bbb --zone=trusted --permanent
firewall-cmd --reload
firewall-cmd --list-all --zone=trusted
```

**Note:** If we did not specify the name of the veth interface on the
profile, then each time the container is started it would get a random
name, and the firewall configuration would not work.

Now connection to the internet should work:

```bash
lxc exec bbb -- ping 169.254.0.1
lxc exec bbb -- ping 8.8.8.8
```

However there is still something that does not work: from outside the
host we cannot ping to the container. The problem is that the traffic
goes through the FORWARD chain of `iptables` and the firewall
currently blocks it. To fix this we should add some rules that allow
forwarding for all the traffic that goes to the interface `veth-bbb`:

```bash
firewall-cmd --permanent --direct --add-rule \
        ipv4 filter FORWARD 0 -o veth-bbb -j ACCEPT
firewall-cmd --permanent --direct --add-rule \
        ipv6 filter FORWARD 0 -o veth-bbb -j ACCEPT
firewall-cmd --reload
```

We can test that everything works with netcat. On the server run `lxc
exec bbb -- nc -l 443`.  Outside the server run `nc 37.121.182.6
443`. Then every line that is typed outside the server should be
displayed inside the server.

**Note:** If we used nic type **ipvlan** instead of **routed**, then
instead of FORWARD, the relevant chains of iptables in this case would
have been INPUT and OUTPUT and the filter rules above would have been
a bit different.

# Step 6 - Installing BBB inside the container

Let's also see how to install BBB inside the container.

```bash
lxc exec bbb -- bash
wget http://ubuntu.bigbluebutton.org/repo/bigbluebutton.asc -O- | apt-key add -
wget -q https://ubuntu.bigbluebutton.org/bbb-install.sh
chmod +x bbb-install.sh
./bbb-install.sh -v bionic-240 -s bbb.example.org -e email@example.org  -g -w
```

## Add admins and users:

```bash
docker exec greenlight-v2 \
    bundle exec rake \
    admin:create["Full Name 1","email1@example.org","passw1","username1"]
docker exec greenlight-v2 \
    bundle exec rake \
    user:create["Full Name 2","email2@example.org","passw2","username2"]
```

## Fix the services

If you run `bbb-conf --status` you will notice that some services are
not working. They can be fixed like this:

```bash
# Override /lib/systemd/system/freeswitch.service
mkdir /etc/systemd/system/freeswitch.service.d
cat <<EOF | tee /etc/systemd/system/freeswitch.service.d/override.conf
[Service]
CPUSchedulingPolicy=other
EOF

# override /usr/lib/systemd/system/bbb-html5-frontend@.service
mkdir /etc/systemd/system/bbb-html5-frontend@.service.d
cat <<EOF | tee /etc/systemd/system/bbb-html5-frontend@.service.d/override.conf
[Service]
CPUSchedulingPolicy=other
EOF

# override /usr/lib/systemd/system/bbb-html5-backend@.service
mkdir /etc/systemd/system/bbb-html5-backend@.service.d
cat <<EOF | tee /etc/systemd/system/bbb-html5-backend@.service.d/override.conf
[Service]
CPUSchedulingPolicy=other
EOF

systemctl daemon-reload
bbb-conf --restart
bbb-conf --status
```

(Thanks to [this post](https://groups.google.com/g/bigbluebutton-setup/c/77AtQ4Hl7ag/m/0_k76m1xAwAJ))

# Step 7 - Installing a TURN server in another container

In some network restricted sites or development environments, such as
those behind NAT or a firewall that restricts outgoing UDP
connections, users may be unable to make outgoing UDP connections to
your BigBlueButton server.

The TURN protocol is designed to allow UDP-based communication flows
like WebRTC to bypass NAT or firewalls by having the client connect to
the TURN server, and then have the TURN server connect to the
destination on their behalf.

In addition, the TURN server implements the STUN protocol as well,
used to allow direct UDP connections through certain types of
firewalls which otherwise might not work.

Using a TURN server under your control improves the success of
connections to BigBlueButton and also improves user privacy, since
they will no longer be sending IP address information to a public STUN
server.

Because the TURN protocol is not CPU and memory intensive, and because
it needs to use the port 443, it makes sense to use another public IP
for it and to install it in a container with a routed NIC.

## Create the profile

We can copy and modify the profile for BBB:

```bash
lxc profile copy bbb turn
lxc profile ls
lxc profile edit turn
```

Change the public IP and the `host_name`. It should look like this:

```yaml
config:
  security.nesting: "true"
  user.network-config: |
    version: 2
    ethernets:
        eth0:
            addresses:
            - 37.121.183.102/32
            nameservers:
                addresses:
                - 8.8.8.8
                search: []
            routes:
            -   to: 0.0.0.0/0
                via: 169.254.0.1
                on-link: true
description: Routed LXD profile
devices:
  eth0:
    host_name: veth-turn
    ipv4.address: 37.121.183.102
    nictype: routed
    parent: eth0
    type: nic
name: turn
used_by:
```

The modifications are these:

```yaml
config:
    ethernets:
        eth0:
            addresses:
            - 37.121.183.102/32

devices:
  eth0:
    host_name: veth-turn
    ipv4.address: 37.121.183.102
```

The setting : **`security.nesting: "true"`** is not actually needed
because we don't need to run docker inside the container, but it
doesn't harm.

## Launch the container

```bash
lxc launch ubuntu:20.04 turn --profile default --profile turn
lxc list
lxc info turn

ip addr show veth-turn
ip ro | grep veth

lxc exec turn -- ip addr
lxc exec turn -- ip ro
```

### Fix networking

```bash
firewall-cmd --permanent --zone=trusted --add-interface=veth-turn 

firewall-cmd --permanent --direct --add-rule \
        ipv4 filter FORWARD 0 -o veth-turn -j ACCEPT
firewall-cmd --permanent --direct --add-rule \
        ipv6 filter FORWARD 0 -o veth-turn -j ACCEPT

firewall-cmd --reload

firewall-cmd --zone=trusted --list-all
iptables-save | grep veth

lxc exec turn -- ping 169.254.0.1
lxc exec turn -- ping 8.8.8.8
```

Use `netcat` as well to make sure that you can reach any TCP or UDP
port inside the container.

### Install `coturn` inside the container

```bash
lxc exec turn -- bash

wget -qO- https://ubuntu.bigbluebutton.org/bbb-install.sh \
    | bash -s -- -c turn.example.com:1234abcd -e info@example.com
```

For more details see:
https://github.com/bigbluebutton/bbb-install#install-a-turn-server

Now reinstall BBB, adding the option `-c turn.example.com:1234abcd` to
the installation command, like this:

```bash
lxc exec bbb -- bash

./bbb-install.sh -g -w \
        -v bionic-240 \
        -s bbb.example.org \
        -e email@example.org  \
        -c turn.example.com:1234abcd
```

For testing that everything works as expected, see:
https://docs.bigbluebutton.org/admin/setup-turn-server.html#test-your-turn-server


# License
MIT

# Contributor's Certificate of Origin
By making a contribution to this project, I certify that:

 1) The contribution was created in whole or in part by me and I have the right to submit it under the license indicated in the file; or

 2) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same license (unless I am permitted to submit under a different license), as indicated in the file; or

 3) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.

 4) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the license(s) involved.

Signed off by: Dashamir Hoxha <dashohoxha@gmail.com>
